{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CreateDocuments import load_documents\n",
    "import RAG_utils\n",
    "\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = RAG_utils.create_chroma_db(doc_path='documents/test_size_1000_overlap_200_documents.pkl', collection_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = RAG_utils.access_chroma_db(collection_name='train_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOUSTON (Jan. 23, 2018) – Fabien Gabel, music ...</td>\n",
       "      <td>Who is the music director of the Quebec Sympho...</td>\n",
       "      <td>The music director of the Quebec Symphony Orch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Port Harcourt – The Rivers police command on W...</td>\n",
       "      <td>Who were the four students of the University o...</td>\n",
       "      <td>The four students of the University of Port Ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  HOUSTON (Jan. 23, 2018) – Fabien Gabel, music ...   \n",
       "1  Port Harcourt – The Rivers police command on W...   \n",
       "\n",
       "                                            question  \\\n",
       "0  Who is the music director of the Quebec Sympho...   \n",
       "1  Who were the four students of the University o...   \n",
       "\n",
       "                                              answer  \n",
       "0  The music director of the Quebec Symphony Orch...  \n",
       "1  The four students of the University of Port Ha...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('rag-dataset-12000/data/test-00000-of-00001-af2a9f454ad1b8a3.parquet')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate rows\n",
    "rows = []\n",
    "for i, row in tqdm.tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    idx = i\n",
    "    question = row['question']\n",
    "    ground_truth_answer = row['answer']\n",
    "\n",
    "    system_message = \"\"\"You are a helpful assistant. Answer the user's question in one sentence based on the provided context. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. Do NOT start your response with \"According to the provided context.\" \"\"\"\n",
    "    user_message_template = \"\"\"Context: {context} Question: {question}\"\"\"\n",
    "\n",
    "    documents = db.similarity_search_with_relevance_scores(question, k=5)\n",
    "    contexts = [doc[0].page_content for doc in documents]\n",
    "    #context = RAG_utils.format_docs([doc[0] for doc in documents])\n",
    "\n",
    "    user_message = user_message_template.format(context=contexts, question=question)\n",
    "\n",
    "    answer = RAG_utils.gen_text_ollama(sys_msg=system_message, user_msg=user_message,options={'seed':0, 'temperature':0.01})\n",
    "\n",
    "    row = [idx, question, answer, contexts, ground_truth_answer]\n",
    "    rows.append(row)\n",
    "    # eval_df = pd.concat([eval_df, pd.DataFrame([row])], ignore_index=True)\n",
    "    if i > 2:\n",
    "        break\n",
    "\n",
    "eval_df = pd.DataFrame(rows, columns=['index','question', 'answer', 'contexts', 'ground_truth'])\n",
    "eval_df\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['contexts'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_correctness, answer_relevancy, context_recall, context_entity_recall\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# langchain_llm = Ollama(model=\"llama3\")\n",
    "# langchain_embeddings = OllamaEmbeddings()\n",
    "\n",
    "results = evaluate(dataset, metrics=[faithfulness, answer_correctness, context_recall, context_entity_recall, answer_relevancy], llm=Ollama(model=\"llama3\"), embeddings=OllamaEmbeddings())\n",
    "results.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_pandas().to_csv('RAGAS_test_eval_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_pandas().iloc[1]['ground_truth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.ragas.io/en/latest/concepts/metrics/context_relevancy.html\n",
    "from ragas.metrics import ContextRelevancy\n",
    "context_relevancy = ContextRelevancy()\n",
    "\n",
    "\n",
    "# Dataset({\n",
    "#     features: ['question','contexts'],\n",
    "#     num_rows: 25\n",
    "# })\n",
    "dataset: Dataset\n",
    "\n",
    "results = context_relevancy.score(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "question = df.iloc[i]['question']\n",
    "ground_truth_answer = df.iloc[i]['answer']\n",
    "\n",
    "system_message = \"\"\"You are a helpful assistant. Answer the user's question in one sentence based on the provided context. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. Do NOT start your response with \"According to the provided context.\" \"\"\"\n",
    "user_message_template = \"\"\"Context: {context} Question: {question}\"\"\"\n",
    "\n",
    "documents = db.similarity_search_with_relevance_scores(question, k=5)\n",
    "context = RAG_utils.format_docs([doc[0] for doc in documents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "context = RAG_utils.format_docs([doc[0] for doc in documents])\n",
    "user_message = user_message_template.format(context=context, question=question)\n",
    "\n",
    "answer = RAG_utils.gen_text_ollama(sys_msg=system_message, user_msg=user_message,options={'seed':0, 'temperature':0.01})\n",
    "print(question)\n",
    "print(answer)\n",
    "\n",
    "score = evaluator.evaluate(context=context, ground_truth_answer=ground_truth_answer, generated_response=answer)\n",
    "score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When was the house at 3524 Redwing Ct, Naperville, IL 60564 last sold and for what price?\n",
      "\n",
      "The house at 3524 Redwing Ct, Naperville, IL 60564 was last sold in October 2013 for $595,000, as provided in the given context.\n"
     ]
    }
   ],
   "source": [
    "question = 'When was the house at 3524 Redwing Ct, Naperville, IL 60564 last sold and for what price?'\n",
    "\n",
    "# prompt_template1 = \"\"\"Your are a helpful assistant. Please answer in one sentence. Answer the question based only on the following context:\n",
    "# {context}\n",
    "# Question: {question}\n",
    "# Answer: \n",
    "# \"\"\"\n",
    "\n",
    "sys_msg = \"\"\"You are a helpful assistant. Answer the user's question in one sentence based on the provided context. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. Do NOT start your response with \"According to the provided context.\" \"\"\"\n",
    "zephyr_prompt_template = \"\"\"<system>{sys_msg}</s>\\n<user>{user_msg}</s>\\n<|assistant|>\"\"\"\n",
    "user_msg_template = \"\"\"Context: {context} Question: {question}\"\"\"\n",
    "\n",
    "\n",
    "documents = db.similarity_search_with_relevance_scores(question, k=5)\n",
    "context = RAG_utils.format_docs([doc[0] for doc in documents])\n",
    "\n",
    "user_msg = user_msg_template.format(context=context, question=question)\n",
    "prompt_text = zephyr_prompt_template.format(sys_msg=sys_msg, user_msg=user_msg)\n",
    "\n",
    "answer = RAG_utils.gen_text_hf_api(lm_name='HuggingFaceH4/zephyr-7b-beta', prompt_text=prompt_text)\n",
    "print(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df = pd.read_parquet('rag-dataset-12000/data/train-00000-of-00001-9df3a936e1f63191.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Bedrock\n",
    "https://docs.ragas.io/en/stable/howtos/customisations/aws-bedrock.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
