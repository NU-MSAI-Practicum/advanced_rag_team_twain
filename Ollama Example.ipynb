{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CreateDocuments import load_documents\n",
    "import RAG_utils\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 10\n",
      "There are 10 in the collection\n"
     ]
    }
   ],
   "source": [
    "db = RAG_utils.create_chroma_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When was the house at 3524 Redwing Ct, Naperville, IL 60564 last sold and for what price?\n",
      "The house at 3524 Redwing Ct, Naperville, IL 60564 was last sold in October 2013 for $595,000.\n"
     ]
    }
   ],
   "source": [
    "question = 'When was the house at 3524 Redwing Ct, Naperville, IL 60564 last sold and for what price?'\n",
    "\n",
    "system_message = \"\"\"You are a helpful assistant. Answer the user's question in one sentence based on the provided context. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. Do NOT start your response with \"According to the provided context.\" \"\"\"\n",
    "user_message_template = \"\"\"Context: {context} Question: {question}\"\"\"\n",
    "\n",
    "documents = db.similarity_search_with_relevance_scores(question, k=5)\n",
    "context = RAG_utils.format_docs([doc[0] for doc in documents])\n",
    "user_message = user_message_template.format(context=context, question=question)\n",
    "\n",
    "answer = RAG_utils.gen_text_ollama(sys_msg=system_message, user_msg=user_message,options={'seed':0, 'temperature':0.01})\n",
    "print(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative - format entire prompt as user message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When was the house at 3524 Redwing Ct, Naperville, IL 60564 last sold and for what price?\n",
      "The house at 3524 Redwing Ct, Naperville, IL 60564 was last sold in October 2013 for $595,000.\n"
     ]
    }
   ],
   "source": [
    "question = 'When was the house at 3524 Redwing Ct, Naperville, IL 60564 last sold and for what price?'\n",
    "\n",
    "# https://huggingface.co/blog/llama2#how-to-prompt-llama-2\n",
    "llama_3_prompt_template = \"\"\"<s>[INST] <<SYS>>:\n",
    "You are a helpful assistant. Answer the user's question in one sentence based on the provided context. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. Do not preface with \"According to the provided context.\"\n",
    "<</SYS>>\n",
    "\n",
    "Context: {context} Question: {question} [/INST]\"\"\"\n",
    "documents = db.similarity_search_with_relevance_scores(question, k=5)\n",
    "context = RAG_utils.format_docs([doc[0] for doc in documents])\n",
    "prompt_text = llama_3_prompt_template.format(context=context, question=question)\n",
    "\n",
    "answer = RAG_utils.gen_text_ollama_user_only(prompt_text, )\n",
    "\n",
    "print(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "5 14\n",
      "10 28\n",
      "15 63\n",
      "20 81\n",
      "25 109\n",
      "30 136\n",
      "35 157\n",
      "40 174\n",
      "45 187\n",
      "50 231\n",
      "55 232\n",
      "60 262\n",
      "65 265\n",
      "70 287\n",
      "75 324\n",
      "80 355\n",
      "85 361\n",
      "90 413\n",
      "95 400\n"
     ]
    }
   ],
   "source": [
    "prompt_text = 'tell me aboout your day'\n",
    "for np in range(0, 100, 5):\n",
    "    response = ollama.generate(model='llama3', prompt=prompt_text, options={'temperature': 0.1, 'seed': 1, 'num_predict': np})\n",
    "    print(np, len(response['response']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.0\n",
      "0.0 757\n",
      "Temperature: 0.05\n",
      "0.05 757\n",
      "Temperature: 0.1\n",
      "0.1 771\n",
      "Temperature: 0.15\n",
      "0.15 815\n",
      "Temperature: 0.2\n",
      "0.2 651\n",
      "Temperature: 0.25\n",
      "0.25 1019\n",
      "Temperature: 0.3\n",
      "0.3 706\n",
      "Temperature: 0.35\n",
      "0.35 1099\n",
      "Temperature: 0.4\n",
      "0.4 866\n",
      "Temperature: 0.45\n",
      "0.45 949\n",
      "Temperature: 0.5\n",
      "0.5 880\n",
      "Temperature: 0.55\n",
      "0.55 975\n",
      "Temperature: 0.6\n",
      "0.6 822\n",
      "Temperature: 0.65\n",
      "0.65 804\n",
      "Temperature: 0.7\n",
      "0.7 843\n",
      "Temperature: 0.75\n",
      "0.75 992\n",
      "Temperature: 0.8\n",
      "0.8 890\n",
      "Temperature: 0.85\n",
      "0.85 974\n",
      "Temperature: 0.9\n",
      "0.9 761\n",
      "Temperature: 0.95\n",
      "0.95 1240\n"
     ]
    }
   ],
   "source": [
    "prompt_text = 'tell me aboout your day'\n",
    "for temp in range(0, 100, 5):\n",
    "    response = ollama.generate(model='llama3', prompt=prompt_text, options={'temperature': temp/100, 'seed': 11})\n",
    "    print(f\"Temperature: {temp/100}\")\n",
    "    print(temp/100, len(response['response']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"options\": {\n",
    "    \"num_keep\": 5,\n",
    "    \"seed\": 42,\n",
    "    \"num_predict\": 100,\n",
    "    \"top_k\": 20,\n",
    "    \"top_p\": 0.9,\n",
    "    \"tfs_z\": 0.5,\n",
    "    \"typical_p\": 0.7,\n",
    "    \"repeat_last_n\": 33,\n",
    "    \"temperature\": 0.8,\n",
    "    \"repeat_penalty\": 1.2,\n",
    "    \"presence_penalty\": 1.5,\n",
    "    \"frequency_penalty\": 1.0,\n",
    "    \"mirostat\": 1,\n",
    "    \"mirostat_tau\": 0.8,\n",
    "    \"mirostat_eta\": 0.6,\n",
    "    \"penalize_newline\": true,\n",
    "    \"stop\": [\"\\n\", \"user:\"],\n",
    "    \"numa\": false,\n",
    "    \"num_ctx\": 1024,\n",
    "    \"num_batch\": 2,\n",
    "    \"num_gqa\": 1,\n",
    "    \"num_gpu\": 1,\n",
    "    \"main_gpu\": 0,\n",
    "    \"low_vram\": false,\n",
    "    \"f16_kv\": true,\n",
    "    \"vocab_only\": false,\n",
    "    \"use_mmap\": true,\n",
    "    \"use_mlock\": false,\n",
    "    \"rope_frequency_base\": 1.1,\n",
    "    \"rope_frequency_scale\": 0.8,\n",
    "    \"num_thread\": 8\n",
    "  }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
